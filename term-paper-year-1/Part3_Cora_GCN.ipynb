{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This will load the network's adjacency matrix A as a Scipy sparse matrix of shape _(N, N)_, the node features X of shape _(N, F)_, and the labels y of shape _(N, n_classes)_. The loader will also return some boolean masks to know which nodes belong to the training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading cora dataset\nPre-processing node features\n"
    }
   ],
   "source": [
    "from spektral.datasets import citation\n",
    "data = citation.load_data('cora')\n",
    "A, X, y, train_mask, val_mask, test_mask = data\n",
    "X = X.toarray()\n",
    "\n",
    "N = A.shape[0] # N = 2708\n",
    "F = X.shape[-1] #X.shape = 2708*1433  = 2708 nodes, 1433 features, F = 1433\n",
    "n_classes = y.shape[-1] # n_classes = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "7\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n [0 0 0 ... 1 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n(2708, 7)\n"
    }
   ],
   "source": [
    "print(n_classes)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.layers import GraphConv\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model is no different than building any Keras model, \n",
    "but we will need to provide multiple inputs to the GraphConv layers (namely A and X):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model definition\n",
    "X_in = Input(shape=(F, ))  # This imply expected input will be batches of F-dimensional matrix (F=1433, input features)\n",
    "A_in = Input((N, ), sparse=True)  # IThis imply expected input will be batches of N-dimensional matrix (N=2704, input adjacency), it is a sparse matrix.\n",
    "\n",
    "graph_conv_1 = GraphConv(16, activation='relu')([X_in, A_in])\n",
    "dropout = Dropout(0.5)(graph_conv_1)\n",
    "graph_conv_2 = GraphConv(n_classes, activation='softmax')([dropout, A_in])\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_conv_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x16f291337b8>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "X_in # <tf.Tensor 'input_5:0' shape=(None, 1433) dtype=float32>\n",
    "\n",
    "A_in # <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x1f922542160>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to notice at this point is how we defined the Input layers of our model. \n",
    "\n",
    "Because the \"elements\" of our dataset are the node themselves, we are telling Keras to consider each node as a separate sample, \n",
    "so that the \"batch\" axis is implicitly defined as None.\n",
    "In other words, a sample of the node attributes will be _a row vector of shape (F, )_ and a sample of the adjacency matrix will be _one of its rows of shape (N, )_. (因為只需要睇該node的所有adjecency node)\n",
    "\n",
    "Keep this detail in mind for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the model, we have to pre-process the adjacency matrix to scale the weights of a node's connections according to its degree. \n",
    "\n",
    "In other words, the more a node is connected to others, the less relative importance those connections have. Most GNN layers available in Spektral require their own type of pre-processing in order to work correctly. You can find all necessary tools for pre-processing A in _spektral.utils_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral import utils\n",
    "A = utils.localpooling_filter(A).astype('f4') #Thats all!! f4 = float, 4 byte long. see https://www.geeksforgeeks.org/data-type-object-dtype-numpy-python/ for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<2708x2708 sparse matrix of type '<class 'numpy.float32'>'\n\twith 13264 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 1433)]       0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\ngraph_conv (GraphConv)          (None, 16)           22944       input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 16)           0           graph_conv[0][0]                 \n__________________________________________________________________________________________________\ngraph_conv_1 (GraphConv)        (None, 7)            119         dropout[0][0]                    \n                                                                 input_2[0][0]                    \n==================================================================================================\nTotal params: 23,063\nTrainable params: 23,063\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain on 2708 samples, validate on 2708 samples\nEpoch 1/100\n2708/2708 [==============================] - 2s 780us/sample - loss: 0.1006 - acc: 0.0857 - val_loss: 0.3592 - val_acc: 0.1640\nEpoch 2/100\n2708/2708 [==============================] - 0s 66us/sample - loss: 0.1006 - acc: 0.0714 - val_loss: 0.3592 - val_acc: 0.1920\nEpoch 3/100\n2708/2708 [==============================] - 0s 78us/sample - loss: 0.1005 - acc: 0.2000 - val_loss: 0.3591 - val_acc: 0.2140\nEpoch 4/100\n2708/2708 [==============================] - 0s 66us/sample - loss: 0.1005 - acc: 0.2214 - val_loss: 0.3590 - val_acc: 0.2520\nEpoch 5/100\n2708/2708 [==============================] - 0s 63us/sample - loss: 0.1004 - acc: 0.2786 - val_loss: 0.3589 - val_acc: 0.3060\nEpoch 6/100\n2708/2708 [==============================] - 0s 70us/sample - loss: 0.1004 - acc: 0.3429 - val_loss: 0.3589 - val_acc: 0.3320\nEpoch 7/100\n2708/2708 [==============================] - 0s 66us/sample - loss: 0.1003 - acc: 0.3500 - val_loss: 0.3588 - val_acc: 0.3640\nEpoch 8/100\n2708/2708 [==============================] - 0s 87us/sample - loss: 0.1003 - acc: 0.3714 - val_loss: 0.3587 - val_acc: 0.3840\nEpoch 9/100\n2708/2708 [==============================] - 0s 84us/sample - loss: 0.1003 - acc: 0.4643 - val_loss: 0.3586 - val_acc: 0.4060\nEpoch 10/100\n2708/2708 [==============================] - 0s 64us/sample - loss: 0.1002 - acc: 0.4857 - val_loss: 0.3585 - val_acc: 0.4200\nEpoch 11/100\n2708/2708 [==============================] - 0s 62us/sample - loss: 0.1002 - acc: 0.4714 - val_loss: 0.3585 - val_acc: 0.4500\nEpoch 12/100\n2708/2708 [==============================] - 0s 61us/sample - loss: 0.1001 - acc: 0.4857 - val_loss: 0.3584 - val_acc: 0.4700\nEpoch 13/100\n2708/2708 [==============================] - 0s 64us/sample - loss: 0.1001 - acc: 0.5214 - val_loss: 0.3583 - val_acc: 0.4920\nEpoch 14/100\n2708/2708 [==============================] - 0s 105us/sample - loss: 0.1000 - acc: 0.5071 - val_loss: 0.3581 - val_acc: 0.5100\nEpoch 15/100\n2708/2708 [==============================] - 0s 105us/sample - loss: 0.1000 - acc: 0.5286 - val_loss: 0.3580 - val_acc: 0.5200\nEpoch 16/100\n2708/2708 [==============================] - 0s 68us/sample - loss: 0.0999 - acc: 0.5571 - val_loss: 0.3579 - val_acc: 0.5380\nEpoch 17/100\n2708/2708 [==============================] - 0s 65us/sample - loss: 0.0998 - acc: 0.6071 - val_loss: 0.3578 - val_acc: 0.5680\nEpoch 18/100\n2708/2708 [==============================] - 0s 57us/sample - loss: 0.0997 - acc: 0.6714 - val_loss: 0.3577 - val_acc: 0.5880\nEpoch 19/100\n2708/2708 [==============================] - 0s 54us/sample - loss: 0.0997 - acc: 0.6929 - val_loss: 0.3575 - val_acc: 0.5960\nEpoch 20/100\n2708/2708 [==============================] - 0s 59us/sample - loss: 0.0996 - acc: 0.6500 - val_loss: 0.3574 - val_acc: 0.6140\nEpoch 21/100\n2708/2708 [==============================] - 0s 85us/sample - loss: 0.0996 - acc: 0.6500 - val_loss: 0.3573 - val_acc: 0.6320\nEpoch 22/100\n2708/2708 [==============================] - 0s 64us/sample - loss: 0.0995 - acc: 0.6786 - val_loss: 0.3571 - val_acc: 0.6500\nEpoch 23/100\n2708/2708 [==============================] - 0s 74us/sample - loss: 0.0994 - acc: 0.6929 - val_loss: 0.3570 - val_acc: 0.6660\nEpoch 24/100\n2708/2708 [==============================] - 0s 73us/sample - loss: 0.0993 - acc: 0.6929 - val_loss: 0.3568 - val_acc: 0.6720\nEpoch 25/100\n2708/2708 [==============================] - 0s 79us/sample - loss: 0.0993 - acc: 0.7214 - val_loss: 0.3567 - val_acc: 0.6740\nEpoch 26/100\n2708/2708 [==============================] - 0s 73us/sample - loss: 0.0992 - acc: 0.7500 - val_loss: 0.3566 - val_acc: 0.6800\nEpoch 27/100\n2708/2708 [==============================] - 0s 90us/sample - loss: 0.0990 - acc: 0.7571 - val_loss: 0.3565 - val_acc: 0.6760\nEpoch 28/100\n2708/2708 [==============================] - 0s 66us/sample - loss: 0.0990 - acc: 0.7000 - val_loss: 0.3564 - val_acc: 0.6760\nEpoch 29/100\n2708/2708 [==============================] - 0s 56us/sample - loss: 0.0990 - acc: 0.6714 - val_loss: 0.3562 - val_acc: 0.6820\nEpoch 30/100\n2708/2708 [==============================] - 0s 58us/sample - loss: 0.0989 - acc: 0.7571 - val_loss: 0.3561 - val_acc: 0.6880\nEpoch 31/100\n2708/2708 [==============================] - 0s 56us/sample - loss: 0.0989 - acc: 0.7714 - val_loss: 0.3560 - val_acc: 0.6940\nEpoch 32/100\n2708/2708 [==============================] - 0s 53us/sample - loss: 0.0987 - acc: 0.7500 - val_loss: 0.3559 - val_acc: 0.7020\nEpoch 33/100\n2708/2708 [==============================] - 0s 58us/sample - loss: 0.0986 - acc: 0.8143 - val_loss: 0.3557 - val_acc: 0.7040\nEpoch 34/100\n2708/2708 [==============================] - 0s 59us/sample - loss: 0.0986 - acc: 0.7857 - val_loss: 0.3556 - val_acc: 0.7060\nEpoch 35/100\n2708/2708 [==============================] - 0s 58us/sample - loss: 0.0985 - acc: 0.7429 - val_loss: 0.3555 - val_acc: 0.7000\nEpoch 36/100\n2708/2708 [==============================] - 0s 66us/sample - loss: 0.0985 - acc: 0.7286 - val_loss: 0.3553 - val_acc: 0.7000\nEpoch 37/100\n2708/2708 [==============================] - 0s 76us/sample - loss: 0.0985 - acc: 0.7714 - val_loss: 0.3552 - val_acc: 0.6980\nEpoch 38/100\n2708/2708 [==============================] - 0s 68us/sample - loss: 0.0984 - acc: 0.7857 - val_loss: 0.3550 - val_acc: 0.6960\nEpoch 39/100\n2708/2708 [==============================] - 0s 61us/sample - loss: 0.0982 - acc: 0.7786 - val_loss: 0.3549 - val_acc: 0.6960\nEpoch 40/100\n2708/2708 [==============================] - 0s 57us/sample - loss: 0.0981 - acc: 0.7929 - val_loss: 0.3548 - val_acc: 0.6980\nEpoch 41/100\n2708/2708 [==============================] - 0s 58us/sample - loss: 0.0982 - acc: 0.7357 - val_loss: 0.3546 - val_acc: 0.7040\nEpoch 42/100\n2708/2708 [==============================] - 0s 57us/sample - loss: 0.0980 - acc: 0.7929 - val_loss: 0.3545 - val_acc: 0.7060\nEpoch 43/100\n2708/2708 [==============================] - 0s 57us/sample - loss: 0.0979 - acc: 0.7571 - val_loss: 0.3543 - val_acc: 0.7140\nEpoch 44/100\n2708/2708 [==============================] - 0s 54us/sample - loss: 0.0979 - acc: 0.7929 - val_loss: 0.3542 - val_acc: 0.7160\nEpoch 45/100\n2708/2708 [==============================] - 0s 54us/sample - loss: 0.0977 - acc: 0.7786 - val_loss: 0.3541 - val_acc: 0.7160\nEpoch 46/100\n2708/2708 [==============================] - 0s 59us/sample - loss: 0.0977 - acc: 0.8071 - val_loss: 0.3539 - val_acc: 0.7180\nEpoch 47/100\n2708/2708 [==============================] - 0s 61us/sample - loss: 0.0977 - acc: 0.7571 - val_loss: 0.3538 - val_acc: 0.7220\nEpoch 48/100\n2708/2708 [==============================] - 0s 45us/sample - loss: 0.0974 - acc: 0.8143 - val_loss: 0.3536 - val_acc: 0.7240\nEpoch 49/100\n2708/2708 [==============================] - 0s 45us/sample - loss: 0.0973 - acc: 0.8357 - val_loss: 0.3535 - val_acc: 0.7260\nEpoch 50/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0972 - acc: 0.7929 - val_loss: 0.3533 - val_acc: 0.7280\nEpoch 51/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0973 - acc: 0.7857 - val_loss: 0.3531 - val_acc: 0.7280\nEpoch 52/100\n2708/2708 [==============================] - 0s 45us/sample - loss: 0.0971 - acc: 0.8500 - val_loss: 0.3530 - val_acc: 0.7300\nEpoch 53/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0969 - acc: 0.8143 - val_loss: 0.3528 - val_acc: 0.7280\nEpoch 54/100\n2708/2708 [==============================] - 0s 47us/sample - loss: 0.0968 - acc: 0.8500 - val_loss: 0.3527 - val_acc: 0.7260\nEpoch 55/100\n2708/2708 [==============================] - 0s 45us/sample - loss: 0.0969 - acc: 0.8500 - val_loss: 0.3525 - val_acc: 0.7240\nEpoch 56/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0967 - acc: 0.8429 - val_loss: 0.3523 - val_acc: 0.7240\nEpoch 57/100\n2708/2708 [==============================] - 0s 45us/sample - loss: 0.0967 - acc: 0.7857 - val_loss: 0.3521 - val_acc: 0.7220\nEpoch 58/100\n2708/2708 [==============================] - 0s 64us/sample - loss: 0.0965 - acc: 0.8357 - val_loss: 0.3520 - val_acc: 0.7240\nEpoch 59/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0966 - acc: 0.8071 - val_loss: 0.3518 - val_acc: 0.7300\nEpoch 60/100\n2708/2708 [==============================] - 0s 45us/sample - loss: 0.0962 - acc: 0.9071 - val_loss: 0.3516 - val_acc: 0.7320\nEpoch 61/100\n2708/2708 [==============================] - 0s 47us/sample - loss: 0.0963 - acc: 0.8214 - val_loss: 0.3514 - val_acc: 0.7320\nEpoch 62/100\n2708/2708 [==============================] - 0s 45us/sample - loss: 0.0964 - acc: 0.8500 - val_loss: 0.3513 - val_acc: 0.7300\nEpoch 63/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0963 - acc: 0.8071 - val_loss: 0.3511 - val_acc: 0.7300\nEpoch 64/100\n2708/2708 [==============================] - 0s 48us/sample - loss: 0.0959 - acc: 0.8357 - val_loss: 0.3509 - val_acc: 0.7280\nEpoch 65/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0958 - acc: 0.8143 - val_loss: 0.3508 - val_acc: 0.7260\nEpoch 66/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0956 - acc: 0.8500 - val_loss: 0.3506 - val_acc: 0.7300\nEpoch 67/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0962 - acc: 0.8357 - val_loss: 0.3504 - val_acc: 0.7320\nEpoch 68/100\n2708/2708 [==============================] - 0s 46us/sample - loss: 0.0955 - acc: 0.8786 - val_loss: 0.3503 - val_acc: 0.7340\nEpoch 69/100\n2708/2708 [==============================] - 0s 49us/sample - loss: 0.0956 - acc: 0.8143 - val_loss: 0.3501 - val_acc: 0.7320\nEpoch 70/100\n2708/2708 [==============================] - 0s 55us/sample - loss: 0.0952 - acc: 0.8429 - val_loss: 0.3499 - val_acc: 0.7340\nEpoch 71/100\n2708/2708 [==============================] - 0s 45us/sample - loss: 0.0956 - acc: 0.8071 - val_loss: 0.3497 - val_acc: 0.7340\nEpoch 72/100\n2708/2708 [==============================] - 0s 46us/sample - loss: 0.0953 - acc: 0.7786 - val_loss: 0.3496 - val_acc: 0.7340\nEpoch 73/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0948 - acc: 0.8571 - val_loss: 0.3494 - val_acc: 0.7360\nEpoch 74/100\n2708/2708 [==============================] - 0s 51us/sample - loss: 0.0950 - acc: 0.8500 - val_loss: 0.3492 - val_acc: 0.7360\nEpoch 75/100\n2708/2708 [==============================] - 0s 52us/sample - loss: 0.0951 - acc: 0.8286 - val_loss: 0.3490 - val_acc: 0.7400\nEpoch 76/100\n2708/2708 [==============================] - 0s 46us/sample - loss: 0.0948 - acc: 0.8500 - val_loss: 0.3488 - val_acc: 0.7340\nEpoch 77/100\n2708/2708 [==============================] - 0s 48us/sample - loss: 0.0949 - acc: 0.8357 - val_loss: 0.3487 - val_acc: 0.7380\nEpoch 78/100\n2708/2708 [==============================] - 0s 45us/sample - loss: 0.0944 - acc: 0.8357 - val_loss: 0.3484 - val_acc: 0.7400\nEpoch 79/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0946 - acc: 0.8571 - val_loss: 0.3482 - val_acc: 0.7440\nEpoch 80/100\n2708/2708 [==============================] - 0s 48us/sample - loss: 0.0942 - acc: 0.8214 - val_loss: 0.3480 - val_acc: 0.7420\nEpoch 81/100\n2708/2708 [==============================] - 0s 44us/sample - loss: 0.0941 - acc: 0.8286 - val_loss: 0.3477 - val_acc: 0.7460\nEpoch 82/100\n2708/2708 [==============================] - 0s 46us/sample - loss: 0.0939 - acc: 0.8643 - val_loss: 0.3475 - val_acc: 0.7520\nEpoch 83/100\n2708/2708 [==============================] - 0s 47us/sample - loss: 0.0937 - acc: 0.8786 - val_loss: 0.3472 - val_acc: 0.7480\nEpoch 84/100\n2708/2708 [==============================] - 0s 46us/sample - loss: 0.0940 - acc: 0.8643 - val_loss: 0.3470 - val_acc: 0.7500\nEpoch 85/100\n2708/2708 [==============================] - 0s 59us/sample - loss: 0.0943 - acc: 0.8214 - val_loss: 0.3468 - val_acc: 0.7500\nEpoch 86/100\n2708/2708 [==============================] - 0s 50us/sample - loss: 0.0939 - acc: 0.8643 - val_loss: 0.3466 - val_acc: 0.7440\nEpoch 87/100\n2708/2708 [==============================] - 0s 49us/sample - loss: 0.0934 - acc: 0.8643 - val_loss: 0.3464 - val_acc: 0.7480\nEpoch 88/100\n2708/2708 [==============================] - 0s 50us/sample - loss: 0.0939 - acc: 0.8500 - val_loss: 0.3462 - val_acc: 0.7460\nEpoch 89/100\n2708/2708 [==============================] - 0s 46us/sample - loss: 0.0935 - acc: 0.9000 - val_loss: 0.3460 - val_acc: 0.7500\nEpoch 90/100\n2708/2708 [==============================] - 0s 46us/sample - loss: 0.0935 - acc: 0.7643 - val_loss: 0.3458 - val_acc: 0.7440\nEpoch 91/100\n2708/2708 [==============================] - 0s 48us/sample - loss: 0.0930 - acc: 0.8286 - val_loss: 0.3456 - val_acc: 0.7480\nEpoch 92/100\n2708/2708 [==============================] - 0s 52us/sample - loss: 0.0929 - acc: 0.8571 - val_loss: 0.3454 - val_acc: 0.7500\nEpoch 93/100\n2708/2708 [==============================] - 0s 47us/sample - loss: 0.0933 - acc: 0.8571 - val_loss: 0.3452 - val_acc: 0.7520\nEpoch 94/100\n2708/2708 [==============================] - 0s 46us/sample - loss: 0.0925 - acc: 0.8214 - val_loss: 0.3450 - val_acc: 0.7520\nEpoch 95/100\n2708/2708 [==============================] - 0s 47us/sample - loss: 0.0926 - acc: 0.8571 - val_loss: 0.3448 - val_acc: 0.7520\nEpoch 96/100\n2708/2708 [==============================] - 0s 65us/sample - loss: 0.0925 - acc: 0.8500 - val_loss: 0.3446 - val_acc: 0.7520\nEpoch 97/100\n2708/2708 [==============================] - 0s 57us/sample - loss: 0.0924 - acc: 0.8714 - val_loss: 0.3445 - val_acc: 0.7520\nEpoch 98/100\n2708/2708 [==============================] - 0s 47us/sample - loss: 0.0923 - acc: 0.8357 - val_loss: 0.3443 - val_acc: 0.7520\nEpoch 99/100\n2708/2708 [==============================] - 0s 52us/sample - loss: 0.0921 - acc: 0.8714 - val_loss: 0.3441 - val_acc: 0.7520\nEpoch 100/100\n2708/2708 [==============================] - 0s 58us/sample - loss: 0.0918 - acc: 0.8714 - val_loss: 0.3439 - val_acc: 0.7520\nEvaluating model.\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\n2708/2708 [==============================] - 0s 16us/sample - loss: 0.6860 - acc: 0.7750\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[0.6860324144363403, 0.775]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Train model\n",
    "validation_data = ([X, A], y, val_mask)\n",
    "model.fit([X, A],\n",
    "          y,\n",
    "          sample_weight=train_mask,\n",
    "          epochs=100,\n",
    "          batch_size=N, #batch size = no of nodes. Put all nodes into neural network at once.\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[\n",
    "              EarlyStopping(patience=10,  restore_best_weights=True)\n",
    "          ])\n",
    "\n",
    "# Evaluate model\n",
    "print('Evaluating model.')\n",
    "eval_results = model.evaluate([X, A],\n",
    "                              y,\n",
    "                              sample_weight=test_mask,\n",
    "                              batch_size=N)\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done.\nTest loss: 0.6860324144363403\nTest accuracy: 0.7749999761581421\n"
    }
   ],
   "source": [
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "      'Test accuracy: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.5727096486108213\n0.5585460768754362\n0.7843426883308715\n[3, 4, 4, 0, 3, 2, 0, 3, 3, 2, 0, 0, 4, 3, 3, 3, 2, 3, 1, 3, 5, 3, 4, 6, 3, 3, 6, 3, 2, 4, 3, 6, 0, 4, 2, 0, 1, 5, 4, 4, 3, 6, 6, 4, 3, 3, 2, 5, 3, 4, 5, 3, 0, 2, 1, 4, 6, 3, 2, 2, 0, 0, 0, 4, 2, 0, 4, 5, 2, 6, 5, 2, 2, 2, 0, 4, 5, 6, 4, 0, 0, 0, 4, 2, 4, 1, 4, 6, 0, 4, 2, 4, 6, 6, 0, 0, 6, 5, 0, 6, 0, 2, 1, 1, 1, 2, 6, 5, 6, 1, 2, 2, 1, 5, 5, 5, 6, 5, 6, 5, 5, 1, 6, 6, 1, 5, 1, 6, 5, 5, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1, 4, 3, 0, 3, 6, 6, 0, 3, 4, 0, 3, 4, 4, 1, 2, 2, 2, 3, 3, 3, 3, 0, 4, 5, 0, 3, 4, 3, 3, 3, 2, 3, 3, 2, 2, 6, 1, 4, 3, 3, 3, 6, 3, 3, 3, 3, 0, 4, 2, 2, 6, 5, 3, 5, 4, 0, 4, 3, 4, 4, 3, 3, 2, 4, 0, 3, 2, 3, 3, 4, 4, 0, 3, 6, 0, 3, 3, 4, 3, 3, 5, 2, 3, 2, 4, 1, 3, 2, 2, 3, 3, 3, 3, 5, 1, 3, 1, 3, 5, 0, 3, 5, 0, 4, 2, 4, 2, 4, 4, 5, 4, 3, 5, 3, 3, 4, 3, 0, 4, 5, 0, 3, 6, 2, 5, 5, 5, 3, 2, 3, 0, 4, 5, 3, 0, 4, 0, 3, 3, 0, 0, 3, 5, 4, 4, 3, 4, 3, 3, 2, 2, 3, 0, 3, 1, 3, 2, 3, 3, 4, 5, 2, 1, 1, 0, 0, 1, 6, 1, 3, 3, 3, 2, 3, 3, 0, 3, 4, 1, 3, 4, 3, 2, 0, 0, 4, 2, 3, 2, 1, 4, 6, 3, 2, 0, 3, 3, 2, 3, 4, 4, 2, 1, 3, 5, 3, 2, 0, 4, 5, 1, 3, 3, 2, 0, 2, 4, 2, 2, 2, 5, 4, 4, 2, 2, 0, 3, 2, 4, 4, 5, 5, 1, 0, 3, 4, 5, 3, 4, 5, 3, 4, 3, 3, 1, 4, 3, 3, 5, 2, 3, 2, 5, 5, 4, 3, 3, 3, 3, 1, 5, 3, 3, 2, 6, 0, 1, 3, 0, 1, 5, 3, 6, 3, 6, 0, 3, 3, 3, 5, 4, 3, 4, 0, 5, 2, 1, 2, 4, 4, 4, 4, 3, 3, 0, 4, 3, 0, 5, 2, 0, 5, 4, 4, 4, 3, 0, 6, 5, 2, 4, 5, 1, 3, 5, 3, 0, 3, 5, 1, 1, 0, 3, 4, 2, 6, 2, 0, 5, 3, 4, 6, 5, 3, 5, 0, 1, 3, 0, 5, 2, 2, 3, 5, 1, 0, 3, 1, 4, 2, 5, 6, 4, 2, 2, 6, 0, 0, 4, 6, 3, 2, 0, 3, 6, 1, 6, 3, 1, 3, 3, 3, 3, 2, 5, 4, 5, 5, 3, 1, 3, 3, 4, 4, 2, 0, 2, 0, 5, 4, 0, 0, 3, 2, 2, 2, 2, 6, 4, 6, 5, 5, 1, 0, 0, 4, 3, 3, 1, 3, 6, 6, 2, 3, 3, 3, 1, 2, 2, 5, 4, 3, 2, 1, 2, 2, 3, 2, 3, 2, 3, 3, 0, 5, 3, 3, 3, 4, 5, 3, 2, 1, 4, 4, 4, 4, 0, 5, 4, 1, 3, 0, 3, 4, 6, 3, 6, 3, 3, 3, 6, 3, 4, 3, 6, 3, 0, 3, 1, 2, 5, 6, 5, 2, 0, 2, 2, 3, 3, 0, 3, 5, 3, 4, 0, 3, 2, 4, 5, 2, 3, 2, 2, 3, 5, 2, 0, 3, 4, 3, 3, 3, 0, 5, 5, 5, 5, 5, 5, 3, 2, 0, 4, 3, 4, 1, 1, 2, 3, 0, 1, 5, 3, 6, 3, 4, 0, 0, 5, 3, 3, 5, 2, 3, 3, 4, 5, 4, 3, 0, 0, 3, 6, 1, 2, 1, 2, 2, 4, 2, 3, 4, 3, 0, 5, 3, 3, 3, 4, 3, 3, 5, 6, 5, 2, 4, 4, 0, 3, 5, 3, 0, 6, 3, 4, 4, 3, 0, 0, 1, 5, 2, 3, 2, 6, 0, 4, 3, 5, 3, 0, 0, 2, 0, 0, 5, 0, 5, 0, 5, 4, 1, 2, 3, 2, 3, 3, 5, 2, 4, 5, 0, 2, 0, 2, 5, 3, 2, 2, 4, 2, 4, 2, 0, 2, 3, 3, 0, 3, 0, 3, 0, 6, 1, 4, 3, 4, 0, 6, 6, 4, 3, 4, 4, 3, 3, 4, 4, 3, 4, 3, 3, 3, 5, 0, 3, 2, 2, 4, 3, 2, 5, 4, 5, 4, 4, 2, 5, 4, 0, 4, 3, 3, 4, 4, 0, 5, 2, 3, 2, 2, 3, 5, 2, 2, 2, 5, 3, 4, 1, 6, 1, 3, 3, 1, 3, 3, 4, 0, 0, 5, 3, 0, 3, 5, 3, 3, 6, 2, 4, 6, 0, 0, 2, 4, 3, 4, 4, 0, 2, 2, 0, 4, 0, 1, 3, 3, 2, 3, 3, 3, 2, 4, 0, 3, 3, 1, 3, 5, 3, 0, 2, 2, 2, 4, 5, 3, 1, 0, 2, 5, 6, 3, 4, 3, 0, 5, 0, 6, 3, 3, 0, 2, 5, 5, 2, 4, 6, 6, 3, 1, 4, 4, 5, 3, 2, 3, 0, 3, 2, 3, 6, 4, 3, 4, 5, 3, 3, 3, 2, 3, 2, 3, 2, 4, 5, 2, 1, 3, 6, 5, 5, 3, 4, 3, 1, 4, 4, 0, 4, 6, 2, 3, 3, 4, 6, 4, 2, 1, 3, 3, 3, 3, 4, 0, 0, 0, 3, 1, 2, 2, 5, 3, 5, 3, 0, 2, 2, 2, 3, 1, 3, 3, 4, 4, 2, 3, 3, 3, 0, 3, 6, 0, 6, 3, 5, 4, 3, 2, 2, 3, 4, 3, 2, 3, 3, 0, 2, 0, 1, 4, 1, 4, 0, 3, 4, 3, 3, 4, 3, 3, 4, 5, 3, 3, 0, 3, 6, 5, 5, 2, 3, 5, 2, 2, 2, 0, 2, 2, 5, 2, 2, 0, 5, 3, 1, 4, 0, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 0, 3, 5, 4, 3, 4, 4, 3, 3, 2, 4, 0, 2, 4, 2, 3, 6, 3, 6, 5, 0, 0, 3, 4, 4, 0, 3, 6, 3, 4, 1, 1, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 4, 2, 0, 5, 3, 3, 3, 4, 0, 4, 4, 5, 2, 4, 3, 0, 0, 3, 0, 3, 5, 2, 3, 0, 3, 3, 5, 4, 3, 3, 3, 5, 3, 4, 2, 0, 4, 0, 1, 4, 1, 4, 1, 2, 1, 3, 2, 2, 2, 3, 0, 4, 2, 2, 0, 4, 1, 3, 3, 2, 4, 6, 2, 6, 3, 5, 5, 2, 6, 3, 0, 2, 0, 3, 3, 3, 4, 5, 1, 5, 5, 5, 5, 3, 3, 0, 0, 2, 5, 3, 3, 1, 4, 0, 4, 1, 0, 2, 3, 3, 4, 0, 1, 2, 4, 4, 4, 2, 2, 3, 3, 3, 2, 6, 2, 3, 0, 3, 0, 3, 5, 3, 0, 3, 5, 5, 0, 2, 4, 3, 0, 2, 4, 4, 6, 5, 2, 3, 4, 3, 3, 2, 1, 1, 4, 3, 1, 2, 2, 1, 2, 1, 2, 4, 3, 4, 1, 0, 4, 4, 2, 2, 4, 4, 4, 5, 0, 5, 3, 3, 3, 3, 3, 0, 5, 3, 3, 0, 2, 2, 2, 1, 2, 0, 4, 2, 6, 3, 3, 6, 2, 0, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 1, 2, 2, 4, 2, 5, 3, 5, 5, 5, 5, 3, 3, 2, 4, 3, 4, 3, 4, 3, 5, 3, 3, 6, 6, 3, 0, 3, 0, 6, 3, 1, 4, 1, 5, 2, 3, 0, 4, 4, 3, 2, 1, 3, 3, 4, 4, 6, 0, 5, 5, 3, 3, 0, 2, 6, 5, 2, 6, 3, 3, 3, 4, 1, 5, 4, 6, 3, 6, 2, 0, 5, 0, 5, 2, 4, 4, 4, 3, 2, 2, 4, 3, 6, 0, 2, 4, 0, 3, 3, 5, 0, 6, 0, 2, 6, 3, 4, 6, 3, 5, 3, 4, 2, 5, 5, 0, 3, 2, 3, 5, 5, 0, 4, 4, 4, 6, 6, 4, 3, 3, 4, 2, 2, 4, 4, 2, 2, 3, 2, 3, 0, 5, 4, 3, 3, 3, 5, 3, 4, 2, 3, 3, 3, 1, 4, 3, 4, 4, 3, 4, 5, 3, 3, 3, 1, 3, 4, 3, 3, 6, 3, 2, 0, 0, 3, 5, 2, 3, 3, 4, 0, 6, 3, 5, 3, 2, 4, 6, 2, 4, 6, 2, 6, 3, 2, 1, 4, 2, 4, 5, 6, 3, 3, 3, 2, 5, 6, 3, 3, 6, 1, 2, 0, 3, 2, 4, 3, 5, 2, 3, 0, 2, 0, 4, 4, 2, 0, 4, 0, 0, 6, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 5, 5, 6, 0, 3, 3, 5, 5, 4, 2, 1, 3, 5, 2, 1, 1, 5, 3, 5, 0, 2, 3, 4, 1, 1, 2, 3, 1, 2, 2, 3, 2, 4, 3, 1, 1, 3, 3, 3, 3, 3, 5, 5, 0, 3, 3, 0, 1, 4, 2, 6, 0, 2, 3, 3, 6, 6, 5, 3, 2, 3, 3, 2, 3, 2, 0, 3, 2, 3, 2, 3, 3, 1, 2, 3, 2, 3, 3, 3, 6, 2, 4, 5, 1, 3, 3, 1, 1, 2, 4, 2, 0, 2, 5, 0, 2, 3, 4, 4, 3, 0, 1, 4, 1, 3, 2, 4, 0, 3, 2, 6, 2, 4, 5, 1, 0, 4, 3, 0, 1, 3, 0, 2, 6, 5, 3, 3, 3, 3, 3, 4, 2, 0, 1, 4, 0, 3, 6, 6, 6, 5, 3, 3, 0, 3, 0, 6, 3, 2, 4, 2, 4, 2, 5, 3, 3, 0, 2, 0, 0, 3, 6, 1, 5, 3, 4, 4, 3, 1, 2, 5, 3, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 3, 4, 4, 4, 4, 1, 1, 3, 1, 0, 3, 0, 2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 1, 6, 6, 3, 0, 0, 5, 0, 5, 0, 3, 5, 3, 0, 0, 6, 0, 6, 3, 3, 1, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 4, 4, 4, 0, 3, 3, 2, 5, 5, 5, 5, 6, 5, 5, 5, 5, 0, 4, 4, 4, 0, 0, 5, 0, 0, 6, 6, 6, 6, 6, 6, 0, 0, 0, 0, 3, 0, 0, 0, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 5, 6, 6, 3, 5, 5, 5, 0, 5, 0, 4, 4, 3, 3, 3, 2, 2, 1, 3, 3, 3, 3, 3, 3, 5, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 6, 3, 6, 0, 5, 0, 0, 4, 0, 6, 5, 5, 0, 1, 3, 3, 5, 6, 5, 3, 3, 4, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 1, 1, 0, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 5, 0, 5, 5, 5, 3, 3, 3, 3, 3, 0, 0, 0, 2, 0, 0, 0, 3, 3, 3, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 3, 1, 1, 1, 1, 1, 0, 0, 0, 5, 5, 5, 5, 3, 5, 1, 1, 3, 6, 6, 5, 6, 2, 3, 3, 0, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 4, 3, 3, 4, 0, 6, 0, 6, 6, 0, 0, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 5, 6, 3, 4, 6, 0, 0, 6, 6, 6, 6, 6, 3, 3, 6, 6, 5, 2, 1, 2, 1, 0, 0, 6, 6, 2, 3, 3, 5, 0, 0, 0, 0, 0, 5, 5, 0, 3, 5, 0, 6, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 1, 6, 1, 0, 3, 3, 3, 3, 3, 6, 1, 0, 2, 2, 4, 4, 4, 4, 4, 5, 6, 3, 3, 0, 0, 0, 0, 5, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 0, 3, 4, 4, 4, 1, 1, 3, 1, 1, 5, 1, 3, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 5, 5, 5, 5, 5, 0, 5, 3, 0, 6, 2, 0, 5, 3, 3, 5, 5, 5, 5, 5, 4, 4, 0, 4, 0, 4, 0, 3, 4, 4, 4, 1, 3, 3, 3, 3, 3, 4, 2, 3, 3, 3, 0, 0, 2, 3, 3, 3, 3, 1, 1, 3, 0, 1, 4, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 2, 4, 4, 4, 3, 3, 3, 4, 0, 3, 3, 3, 3, 0, 3, 3, 4, 4, 4, 4, 4, 4, 0, 4, 3, 2, 0, 3, 4, 5, 0, 2, 2, 3, 3, 3, 3, 3, 2, 3, 5, 5, 4, 1, 4, 4, 4, 3, 4, 4, 0, 4, 4, 4, 5, 2, 2, 2, 2, 4, 6, 6, 6, 6, 3, 4, 4, 4, 1, 3, 0, 3, 3, 5, 0, 2, 3, 3, 3, 3, 3, 2, 4, 4, 0, 0, 3, 2, 6, 6, 0, 3, 3, 3, 5, 1, 3, 4, 4, 2, 4, 4, 4, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 6, 6, 5, 6, 6, 3, 2, 6, 3, 4, 4, 4, 2, 6, 6, 0, 0, 3, 0, 4, 4, 3, 2, 3, 1, 6, 6, 5, 3, 4, 3, 5, 3, 1, 1, 3, 4, 5, 2, 3, 3, 3, 4, 5, 4, 0, 3, 3, 0, 2, 1, 1, 5, 2, 3, 3, 5, 0, 2, 3, 2, 2, 5, 5, 4, 3, 4, 3, 2, 2, 4, 2, 4, 5, 5, 3, 2, 3, 1, 0, 3, 3, 4, 5, 4, 3, 3, 3, 3, 3, 0, 1, 2, 4, 4, 4, 3, 3, 3, 5, 2, 3, 2, 2, 2, 3, 2, 2, 0, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3, 0, 3, 0, 2, 3, 4, 1, 2, 5, 4, 3, 3, 3, 1, 5, 3, 4, 3, 2, 2, 1, 3, 3, 3, 3, 3, 6, 3, 3, 3, 6, 3, 3, 3, 2, 3, 2, 4, 2, 4, 2, 2, 1, 5, 6, 4, 3, 3, 3, 2, 5, 3, 3, 4, 3, 3, 3, 3, 3, 4, 6, 0, 3, 2, 2, 2, 5, 4, 4, 4, 4, 6, 3, 2, 2, 0, 2, 2, 2, 2, 2, 3, 4, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 3, 3, 2, 6, 2, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3]\n[3, 4, 4, 0, 3, 2, 0, 3, 3, 2, 0, 0, 4, 3, 3, 3, 2, 3, 1, 3, 5, 3, 4, 6, 3, 3, 6, 3, 2, 4, 3, 6, 6, 4, 2, 0, 1, 0, 4, 4, 3, 6, 6, 4, 3, 3, 2, 5, 3, 3, 5, 3, 0, 2, 1, 0, 6, 3, 2, 2, 0, 0, 0, 4, 2, 0, 4, 5, 2, 6, 5, 2, 2, 2, 0, 4, 5, 6, 4, 0, 0, 0, 4, 2, 4, 1, 4, 6, 0, 4, 2, 4, 6, 6, 0, 0, 6, 5, 0, 6, 0, 2, 1, 1, 1, 2, 6, 5, 6, 1, 2, 2, 1, 5, 5, 5, 6, 5, 6, 5, 5, 1, 6, 6, 1, 5, 1, 6, 5, 5, 5, 1, 5, 1, 1, 1, 1, 1, 1, 1, 4, 4, 0, 3, 6, 6, 0, 6, 4, 3, 3, 4, 4, 1, 2, 2, 2, 3, 3, 3, 3, 6, 0, 5, 0, 3, 4, 0, 0, 3, 2, 3, 4, 2, 2, 6, 1, 4, 3, 3, 3, 6, 3, 3, 3, 3, 0, 4, 2, 2, 6, 1, 2, 5, 4, 0, 4, 3, 4, 4, 3, 3, 2, 4, 5, 3, 2, 3, 3, 4, 0, 0, 3, 6, 0, 3, 3, 4, 3, 3, 2, 2, 1, 2, 3, 6, 3, 2, 2, 3, 3, 6, 3, 5, 1, 3, 1, 3, 5, 0, 4, 5, 0, 4, 2, 4, 2, 0, 4, 5, 1, 3, 6, 3, 3, 6, 2, 0, 4, 5, 2, 3, 6, 2, 5, 5, 0, 6, 2, 3, 0, 4, 0, 2, 0, 4, 0, 4, 2, 0, 0, 3, 5, 4, 4, 6, 4, 3, 3, 2, 2, 4, 0, 4, 1, 3, 2, 3, 3, 4, 0, 2, 1, 1, 0, 0, 1, 6, 1, 4, 3, 3, 2, 3, 3, 0, 3, 4, 1, 2, 5, 3, 2, 0, 0, 0, 2, 3, 2, 1, 4, 6, 4, 2, 0, 3, 3, 2, 3, 0, 4, 2, 1, 3, 5, 3, 2, 0, 4, 0, 1, 3, 3, 2, 6, 2, 0, 2, 2, 2, 5, 2, 4, 2, 2, 2, 3, 2, 4, 4, 5, 5, 1, 0, 6, 4, 2, 4, 4, 5, 2, 4, 3, 4, 1, 4, 3, 3, 6, 1, 3, 2, 5, 5, 4, 3, 3, 1, 6, 1, 5, 1, 3, 2, 5, 0, 1, 4, 0, 1, 5, 3, 6, 3, 6, 0, 3, 1, 3, 0, 4, 3, 4, 0, 5, 2, 1, 2, 4, 0, 4, 4, 3, 5, 0, 3, 0, 0, 0, 2, 0, 0, 4, 4, 4, 3, 6, 6, 5, 2, 4, 5, 1, 4, 6, 3, 0, 3, 5, 1, 3, 3, 2, 4, 2, 0, 2, 3, 5, 3, 4, 0, 5, 4, 0, 0, 1, 2, 5, 5, 3, 2, 3, 5, 1, 0, 3, 1, 5, 2, 0, 6, 4, 2, 2, 6, 6, 2, 0, 6, 3, 2, 6, 4, 4, 1, 1, 3, 1, 6, 3, 4, 4, 2, 0, 4, 0, 2, 6, 1, 3, 3, 1, 4, 2, 0, 2, 0, 5, 4, 1, 3, 3, 2, 2, 2, 2, 6, 0, 6, 5, 5, 1, 0, 0, 4, 3, 3, 0, 4, 6, 6, 2, 6, 6, 3, 1, 2, 2, 5, 4, 3, 2, 1, 2, 2, 3, 2, 3, 2, 3, 0, 0, 1, 4, 3, 4, 0, 0, 3, 2, 6, 4, 4, 4, 4, 0, 5, 4, 1, 5, 0, 0, 0, 6, 3, 6, 0, 3, 3, 4, 2, 4, 3, 6, 3, 3, 3, 1, 2, 1, 1, 5, 2, 2, 2, 2, 4, 3, 0, 3, 5, 3, 4, 0, 3, 2, 4, 2, 2, 2, 2, 2, 3, 0, 2, 0, 3, 4, 3, 3, 2, 2, 5, 6, 6, 5, 5, 5, 3, 2, 0, 4, 6, 4, 1, 1, 2, 3, 6, 1, 0, 3, 6, 3, 4, 0, 0, 5, 3, 6, 5, 2, 3, 4, 0, 5, 4, 4, 0, 0, 3, 5, 1, 2, 6, 2, 2, 2, 2, 3, 4, 3, 0, 5, 0, 0, 3, 4, 1, 3, 5, 0, 5, 2, 4, 4, 0, 3, 5, 3, 6, 6, 3, 4, 4, 3, 3, 0, 1, 5, 2, 3, 2, 3, 0, 4, 2, 6, 3, 6, 0, 2, 0, 0, 5, 0, 0, 3, 5, 4, 1, 2, 0, 2, 3, 3, 5, 2, 0, 2, 0, 2, 5, 2, 0, 0, 2, 2, 4, 2, 4, 2, 0, 2, 3, 3, 1, 2, 0, 3, 0, 0, 1, 4, 3, 3, 1, 6, 6, 3, 3, 4, 4, 3, 3, 4, 0, 3, 4, 3, 4, 3, 5, 4, 3, 2, 2, 4, 3, 2, 5, 4, 5, 4, 4, 2, 5, 4, 0, 4, 3, 3, 0, 1, 0, 5, 2, 5, 2, 2, 1, 6, 2, 2, 2, 5, 0, 4, 6, 6, 1, 2, 3, 1, 3, 3, 4, 0, 0, 5, 3, 6, 1, 5, 3, 6, 6, 2, 4, 6, 0, 5, 2, 5, 3, 4, 5, 0, 2, 2, 0, 4, 4, 1, 0, 3, 2, 0, 3, 4, 2, 3, 1, 3, 3, 2, 3, 5, 1, 4, 2, 2, 2, 4, 0, 3, 1, 0, 2, 5, 6, 3, 4, 3, 0, 2, 0, 6, 3, 3, 0, 2, 6, 5, 2, 4, 6, 0, 3, 1, 4, 4, 6, 3, 2, 3, 2, 3, 2, 3, 6, 4, 3, 4, 3, 3, 3, 3, 2, 2, 2, 3, 2, 0, 5, 2, 1, 3, 6, 5, 6, 3, 1, 3, 6, 4, 4, 0, 2, 6, 2, 2, 0, 4, 6, 4, 2, 1, 3, 1, 3, 3, 4, 0, 0, 0, 3, 3, 2, 2, 5, 3, 1, 6, 1, 2, 2, 2, 3, 1, 3, 3, 4, 0, 2, 3, 3, 3, 6, 3, 6, 6, 0, 3, 3, 4, 3, 2, 2, 3, 4, 3, 2, 2, 3, 0, 2, 0, 1, 4, 1, 4, 0, 3, 0, 3, 0, 4, 3, 3, 6, 5, 3, 3, 0, 3, 5, 5, 6, 2, 3, 5, 2, 2, 2, 6, 2, 2, 5, 2, 2, 0, 3, 3, 1, 4, 0, 2, 3, 4, 4, 3, 2, 3, 3, 1, 0, 0, 2, 5, 4, 3, 4, 4, 1, 2, 2, 4, 0, 2, 4, 1, 3, 6, 3, 6, 2, 2, 0, 3, 4, 4, 3, 3, 6, 3, 4, 1, 1, 3, 3, 3, 3, 4, 3, 1, 4, 3, 3, 3, 6, 4, 2, 0, 0, 3, 3, 3, 4, 0, 4, 4, 5, 2, 0, 3, 0, 0, 0, 0, 3, 5, 2, 3, 0, 3, 3, 5, 0, 3, 0, 3, 5, 3, 4, 2, 0, 4, 0, 0, 4, 5, 4, 1, 2, 1, 3, 2, 2, 2, 3, 0, 4, 2, 2, 0, 4, 1, 3, 4, 2, 4, 6, 2, 6, 3, 2, 5, 2, 5, 3, 0, 2, 0, 3, 3, 3, 0, 5, 1, 0, 5, 5, 5, 2, 3, 0, 0, 2, 5, 3, 4, 1, 4, 0, 1, 1, 3, 2, 3, 3, 4, 0, 0, 2, 4, 4, 4, 2, 2, 3, 3, 3, 2, 3, 2, 3, 0, 4, 0, 3, 3, 3, 0, 3, 5, 5, 0, 2, 4, 6, 0, 2, 4, 3, 6, 5, 2, 0, 4, 3, 6, 2, 0, 1, 5, 3, 3, 2, 2, 1, 6, 1, 2, 4, 3, 0, 1, 0, 4, 4, 2, 6, 4, 4, 4, 5, 0, 5, 3, 3, 3, 3, 0, 0, 5, 5, 4, 0, 2, 2, 1, 1, 2, 5, 0, 2, 5, 1, 3, 6, 2, 0, 3, 3, 0, 3, 3, 3, 3, 3, 0, 3, 1, 2, 2, 4, 2, 0, 6, 1, 5, 6, 5, 3, 3, 2, 4, 3, 4, 3, 4, 3, 4, 3, 3, 6, 6, 4, 1, 3, 5, 0, 3, 1, 4, 1, 3, 2, 3, 2, 4, 6, 3, 2, 1, 3, 3, 4, 4, 6, 1, 5, 5, 3, 3, 0, 2, 6, 5, 2, 6, 3, 3, 4, 4, 1, 3, 4, 6, 3, 6, 2, 2, 5, 0, 6, 2, 4, 4, 4, 3, 2, 2, 4, 2, 5, 0, 2, 4, 0, 3, 3, 0, 0, 0, 0, 2, 0, 3, 4, 6, 3, 5, 3, 4, 6, 5, 5, 0, 1, 2, 3, 5, 5, 0, 4, 4, 4, 6, 6, 4, 2, 4, 4, 3, 2, 4, 4, 2, 2, 3, 2, 3, 0, 4, 2, 3, 3, 3, 0, 1, 4, 2, 3, 3, 3, 1, 2, 3, 4, 4, 3, 4, 5, 6, 6, 3, 1, 2, 4, 3, 0, 6, 3, 2, 0, 0, 4, 5, 2, 4, 3, 0, 0, 5, 3, 5, 3, 2, 0, 6, 1, 4, 6, 2, 6, 3, 2, 1, 4, 2, 4, 5, 6, 6, 4, 1, 2, 5, 6, 3, 3, 6, 1, 1, 6, 3, 2, 4, 5, 1, 2, 4, 0, 2, 0, 4, 0, 2, 0, 4, 0, 0, 6, 0, 0, 2, 0, 4, 4, 4, 4, 4, 0, 0, 5, 5, 6, 0, 3, 3, 5, 2, 4, 2, 1, 3, 5, 2, 1, 1, 5, 3, 5, 0, 2, 3, 4, 1, 1, 2, 3, 1, 2, 2, 3, 2, 4, 0, 1, 1, 3, 2, 5, 3, 3, 5, 5, 0, 3, 3, 6, 1, 4, 2, 4, 0, 2, 4, 3, 6, 6, 6, 3, 2, 3, 3, 2, 2, 2, 0, 3, 2, 3, 2, 3, 0, 1, 2, 3, 2, 3, 3, 3, 6, 2, 4, 5, 2, 4, 1, 1, 1, 2, 3, 2, 0, 2, 5, 0, 2, 3, 4, 0, 3, 6, 1, 4, 1, 3, 2, 4, 0, 6, 2, 6, 2, 4, 5, 1, 0, 4, 3, 0, 1, 3, 5, 2, 6, 6, 3, 3, 3, 3, 3, 0, 2, 4, 1, 0, 3, 0, 6, 6, 6, 0, 2, 3, 0, 1, 0, 6, 3, 2, 4, 2, 2, 2, 6, 3, 0, 0, 2, 0, 0, 3, 6, 1, 5, 1, 4, 4, 3, 1, 2, 5, 1, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 6, 5, 4, 4, 4, 1, 1, 3, 1, 1, 1, 6, 2, 1, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 6, 6, 6, 0, 0, 2, 0, 0, 0, 2, 4, 3, 5, 0, 6, 0, 6, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 0, 4, 4, 4, 3, 0, 2, 5, 5, 5, 1, 6, 5, 5, 5, 5, 4, 4, 0, 0, 0, 1, 1, 0, 0, 6, 6, 6, 6, 1, 6, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 5, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 5, 6, 6, 2, 5, 5, 5, 5, 5, 0, 0, 4, 0, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 1, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3, 6, 3, 0, 0, 5, 0, 6, 0, 0, 5, 0, 0, 6, 6, 0, 0, 1, 3, 6, 5, 6, 6, 4, 3, 0, 4, 3, 3, 4, 4, 4, 3, 4, 4, 4, 1, 1, 1, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 5, 5, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 1, 1, 0, 6, 6, 3, 3, 3, 3, 1, 1, 0, 6, 6, 6, 6, 2, 3, 3, 0, 0, 3, 4, 4, 4, 4, 4, 3, 4, 4, 4, 3, 3, 0, 0, 6, 0, 6, 6, 0, 0, 3, 3, 3, 3, 3, 1, 1, 1, 6, 3, 6, 3, 2, 6, 3, 5, 6, 0, 0, 6, 6, 6, 6, 6, 3, 3, 6, 6, 6, 2, 2, 1, 0, 0, 0, 6, 6, 3, 3, 1, 0, 4, 0, 0, 0, 0, 5, 5, 0, 4, 6, 0, 6, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 3, 3, 3, 3, 2, 6, 1, 4, 3, 3, 3, 3, 3, 6, 1, 0, 2, 2, 4, 4, 4, 4, 4, 0, 0, 0, 3, 0, 0, 0, 0, 5, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 1, 1, 3, 1, 6, 1, 3, 3, 4, 4, 4, 4, 0, 4, 4, 0, 0, 4, 5, 5, 5, 5, 5, 5, 5, 2, 0, 6, 2, 0, 5, 6, 3, 5, 5, 5, 5, 5, 4, 0, 0, 4, 6, 4, 0, 3, 0, 0, 4, 1, 4, 4, 3, 3, 4, 4, 2, 3, 0, 3, 0, 0, 5, 1, 3, 6, 3, 1, 4, 3, 0, 1, 5, 1, 1, 3, 1, 1, 1, 1, 1, 0, 0, 2, 4, 4, 4, 3, 3, 3, 3, 0, 3, 3, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 2, 2, 3, 3, 4, 5, 0, 2, 2, 3, 3, 3, 3, 3, 3, 6, 5, 5, 4, 3, 4, 4, 4, 3, 1, 0, 3, 0, 4, 4, 6, 2, 2, 2, 2, 4, 0, 0, 6, 4, 3, 4, 4, 4, 3, 3, 0, 5, 3, 5, 0, 0, 3, 3, 3, 2, 3, 2, 4, 4, 0, 0, 3, 2, 6, 6, 0, 0, 3, 0, 0, 1, 3, 0, 0, 2, 4, 4, 6, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 1, 1, 2, 6, 6, 6, 2, 6, 6, 2, 2, 6, 6, 4, 4, 4, 2, 5, 5, 2, 2, 3, 0, 4, 4, 3, 2, 3, 1, 6, 6, 5, 1, 4, 4, 6, 3, 1, 1, 3, 0, 5, 2, 3, 3, 3, 4, 4, 4, 0, 3, 3, 0, 2, 1, 1, 5, 2, 3, 3, 3, 0, 2, 3, 2, 2, 6, 6, 4, 3, 4, 3, 6, 2, 4, 2, 0, 5, 5, 3, 2, 3, 3, 3, 3, 3, 4, 5, 4, 3, 3, 3, 3, 3, 0, 0, 2, 3, 4, 4, 3, 3, 4, 1, 2, 3, 2, 2, 2, 3, 2, 0, 6, 4, 4, 0, 0, 0, 3, 2, 6, 2, 6, 6, 3, 0, 0, 3, 3, 3, 0, 2, 3, 0, 2, 2, 5, 4, 3, 2, 2, 1, 5, 3, 4, 3, 2, 2, 6, 3, 2, 3, 3, 3, 6, 3, 2, 2, 6, 3, 3, 0, 2, 3, 2, 4, 2, 4, 2, 2, 0, 5, 6, 4, 3, 3, 3, 2, 5, 3, 5, 4, 3, 3, 3, 3, 3, 4, 6, 2, 5, 2, 2, 2, 5, 4, 4, 4, 4, 6, 3, 2, 2, 0, 2, 1, 6, 2, 2, 3, 0, 4, 4, 3, 3, 0, 0, 3, 3, 0, 4, 0, 4, 4, 4, 4, 3, 4, 4, 4, 4, 0, 4, 2, 4, 2, 3, 3, 3, 2, 1, 2, 6, 3, 4, 4, 5, 3, 3, 3, 3, 3, 0, 3, 2, 3, 3, 3]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# one hot encoder to int\n",
    "ground_truth = [np.where(r==1)[0][0] for r in y]\n",
    "\n",
    "y_result = model.predict([X,A], batch_size=N)\n",
    "y_group = []\n",
    "for index, item in enumerate(y_result):\n",
    "    y_group.append(np.argmax(y_result[index]))\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.adjusted_rand_score(ground_truth, y_group))\n",
    "print(metrics.adjusted_mutual_info_score(ground_truth, y_group))\n",
    "print(metrics.accuracy_score(ground_truth, y_group))\n",
    "print(ground_truth)\n",
    "print(y_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36664bitalexlvirtualenvb9f0b0a3af2a4e06a89ee778b9503914",
   "display_name": "Python 3.6.6 64-bit ('alexl': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}