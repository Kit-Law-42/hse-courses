{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import SpectralClustering, KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "from spektral.layers import GraphConv\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd()\n",
    "edge_location = os.path.expanduser(os.path.join(data_dir, \"fb-CMU-Carnegie49/fb-CMU-Carnegie49.edges\"))\n",
    "g_nx = nx.read_edgelist(path=edge_location)\n",
    "\n",
    "\n",
    "class_data_location = os.path.expanduser(os.path.join(data_dir, \"fb-CMU-Carnegie49/fb-CMU-Carnegie49.node_labels\"))\n",
    "node_attr = pd.read_csv(class_data_location, sep=',', header=None)\n",
    "values = { str(row.tolist()[0]): row.tolist()[-1] for _, row in node_attr.iterrows()}\n",
    "nx.set_node_attributes(g_nx, values, 'class')\n",
    "\n",
    "column_names =  [\"node_id\" ,\"class\"]\n",
    "node_data = pd.read_csv(os.path.join(data_dir, \"fb-CMU-Carnegie49/fb-CMU-Carnegie49.node_labels\"), header=None, names=column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Largest subgraph statistics: 6621 nodes, 249959 edges\nThere are 3 unique labels on the nodes.\nThere are 6621 nodes in the network.\n"
    }
   ],
   "source": [
    "g_nx_ccs = (g_nx.subgraph(c).copy() for c in nx.connected_components(g_nx))\n",
    "g_nx = max(g_nx_ccs, key=len)\n",
    "node_ids = list(g_nx.nodes())\n",
    "print(\"Largest subgraph statistics: {} nodes, {} edges\".format(\n",
    "    g_nx.number_of_nodes(), g_nx.number_of_edges()))\n",
    "\n",
    "node_targets = [ g_nx.nodes[node_id]['class'] for node_id in node_ids]\n",
    "\n",
    "print(f\"There are {len(np.unique(node_targets))} unique labels on the nodes.\")\n",
    "\n",
    "print(f\"There are {len(g_nx.nodes())} nodes in the network.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6621\n"
    }
   ],
   "source": [
    "s = set(node_data[\"class\"])\n",
    "#build a dictionary to convert string to numbers\n",
    "convert_table = {e:idx for idx, e in enumerate(s)}\n",
    "\n",
    "def word2idx(word):\n",
    "    return convert_table[word]\n",
    "\n",
    "ground_truth =  [word2idx(i) for i in node_targets]\n",
    "print(len(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.to_numpy_array(g_nx) \n",
    "X = np.diag(np.ones(len(g_nx.nodes()))) #6621\n",
    "y =  np.zeros((len(ground_truth), max(ground_truth)+1))\n",
    "y[np.arange(len(ground_truth)),ground_truth] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "6621   6621   3\n<class 'numpy.ndarray'>\n"
    }
   ],
   "source": [
    "N = A.shape[0] # N = 6621\n",
    "F = X.shape[-1] #X.shape = 6621*6621 \n",
    "n_classes = y.shape[-1] #3\n",
    "\n",
    "print(N, ' ', F, ' ', n_classes)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[(840, '2110'), (785, '2491'), (777, '3076'), (742, '2785'), (711, '3104'), (642, '6592')]\n"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "degree_sequence = sorted([(d, n) for n, d in g_nx.degree()], reverse=True)\n",
    "print(degree_sequence[0:6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'numpy.ndarray'>\n"
    }
   ],
   "source": [
    "train_mask,  val_mask, test_mask =  (np.zeros(N) for i in range(3))\n",
    "\n",
    "train_mask[0:400] = [1 for i in range(400)]\n",
    "val_mask[400:2000] = [1 for i in range(1600)]\n",
    "test_mask[2000:] = [1 for i in range(N-2000)]\n",
    "\n",
    "print(type(train_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "X_in = Input(shape=(F, ))  # This imply expected input will be batches of F-dimensional matrix (F=1433, input features)\n",
    "A_in = Input((N, ), sparse=True)  # IThis imply expected input will be batches of N-dimensional matrix (N=2704, input adjacency), it is a sparse matrix.\n",
    "\n",
    "graph_conv_1 = GraphConv(128, activation='relu')([X_in, A_in])\n",
    "dropout1 = Dropout(0.3)(graph_conv_1)\n",
    "\n",
    "# graph_conv_2 = GraphConv(12, activation='relu')([dropout1, A_in])\n",
    "# dropout2 = Dropout(0.3)(graph_conv_2)\n",
    "\n",
    "graph_conv_3 = GraphConv(n_classes, activation='softmax')([dropout1, A_in])\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_conv_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral import utils\n",
    "from scipy import sparse\n",
    "A = sparse.csr_matrix(A)\n",
    "A = utils.localpooling_filter(A).astype('f4') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'scipy.sparse.csr.csr_matrix'>\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n"
    }
   ],
   "source": [
    "print(type(A))\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "print(type(train_mask))\n",
    "print(type(val_mask))\n",
    "print(type(test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_6\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_13 (InputLayer)           [(None, 6621)]       0                                            \n__________________________________________________________________________________________________\ninput_14 (InputLayer)           [(None, None)]       0                                            \n__________________________________________________________________________________________________\ngraph_conv_13 (GraphConv)       (None, 128)          847616      input_13[0][0]                   \n                                                                 input_14[0][0]                   \n__________________________________________________________________________________________________\ndropout_6 (Dropout)             (None, 128)          0           graph_conv_13[0][0]              \n__________________________________________________________________________________________________\ngraph_conv_14 (GraphConv)       (None, 3)            387         dropout_6[0][0]                  \n                                                                 input_14[0][0]                   \n==================================================================================================\nTotal params: 848,003\nTrainable params: 848,003\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.5200\nEpoch 71/300\n6621/6621 [==============================] - 3s 425us/sample - loss: 0.0518 - acc: 0.5275 - val_loss: 0.2087 - val_acc: 0.5200\nEpoch 72/300\n6621/6621 [==============================] - 2s 328us/sample - loss: 0.0517 - acc: 0.5250 - val_loss: 0.2085 - val_acc: 0.5200\nEpoch 73/300\n6621/6621 [==============================] - 3s 379us/sample - loss: 0.0516 - acc: 0.5350 - val_loss: 0.2083 - val_acc: 0.5213\nEpoch 74/300\n6621/6621 [==============================] - 2s 356us/sample - loss: 0.0515 - acc: 0.5450 - val_loss: 0.2081 - val_acc: 0.5200\nEpoch 75/300\n6621/6621 [==============================] - 2s 348us/sample - loss: 0.0515 - acc: 0.5300 - val_loss: 0.2079 - val_acc: 0.5206\nEpoch 76/300\n6621/6621 [==============================] - 2s 361us/sample - loss: 0.0515 - acc: 0.5275 - val_loss: 0.2077 - val_acc: 0.5213\nEpoch 77/300\n6621/6621 [==============================] - 2s 363us/sample - loss: 0.0514 - acc: 0.5300 - val_loss: 0.2076 - val_acc: 0.5213\nEpoch 78/300\n6621/6621 [==============================] - 2s 368us/sample - loss: 0.0514 - acc: 0.5275 - val_loss: 0.2074 - val_acc: 0.5200\nEpoch 79/300\n6621/6621 [==============================] - 2s 321us/sample - loss: 0.0514 - acc: 0.5325 - val_loss: 0.2073 - val_acc: 0.5206\nEpoch 80/300\n6621/6621 [==============================] - 2s 362us/sample - loss: 0.0513 - acc: 0.5500 - val_loss: 0.2071 - val_acc: 0.5206\nEpoch 81/300\n6621/6621 [==============================] - 2s 309us/sample - loss: 0.0513 - acc: 0.5350 - val_loss: 0.2070 - val_acc: 0.5213\nEpoch 82/300\n6621/6621 [==============================] - 2s 314us/sample - loss: 0.0512 - acc: 0.5550 - val_loss: 0.2069 - val_acc: 0.5219\nEpoch 83/300\n6621/6621 [==============================] - 3s 384us/sample - loss: 0.0511 - acc: 0.5600 - val_loss: 0.2067 - val_acc: 0.5206\nEpoch 84/300\n6621/6621 [==============================] - 3s 385us/sample - loss: 0.0511 - acc: 0.5600 - val_loss: 0.2066 - val_acc: 0.5219\nEpoch 85/300\n6621/6621 [==============================] - 3s 409us/sample - loss: 0.0511 - acc: 0.5575 - val_loss: 0.2065 - val_acc: 0.5200\nEpoch 86/300\n6621/6621 [==============================] - 2s 322us/sample - loss: 0.0510 - acc: 0.5675 - val_loss: 0.2064 - val_acc: 0.5200\nEpoch 87/300\n6621/6621 [==============================] - 2s 347us/sample - loss: 0.0510 - acc: 0.5725 - val_loss: 0.2063 - val_acc: 0.5219\nEpoch 88/300\n6621/6621 [==============================] - 2s 317us/sample - loss: 0.0510 - acc: 0.5650 - val_loss: 0.2062 - val_acc: 0.5231\nEpoch 89/300\n6621/6621 [==============================] - 2s 310us/sample - loss: 0.0509 - acc: 0.5800 - val_loss: 0.2061 - val_acc: 0.5238\nEpoch 90/300\n6621/6621 [==============================] - 2s 340us/sample - loss: 0.0509 - acc: 0.5850 - val_loss: 0.2061 - val_acc: 0.5244\nEpoch 91/300\n6621/6621 [==============================] - 2s 317us/sample - loss: 0.0509 - acc: 0.5875 - val_loss: 0.2060 - val_acc: 0.5256\nEpoch 92/300\n6621/6621 [==============================] - 2s 332us/sample - loss: 0.0508 - acc: 0.5850 - val_loss: 0.2059 - val_acc: 0.5244\nEpoch 93/300\n6621/6621 [==============================] - 2s 316us/sample - loss: 0.0508 - acc: 0.5850 - val_loss: 0.2058 - val_acc: 0.5244\nEpoch 94/300\n6621/6621 [==============================] - 2s 310us/sample - loss: 0.0508 - acc: 0.5950 - val_loss: 0.2058 - val_acc: 0.5250\nEpoch 95/300\n6621/6621 [==============================] - 2s 340us/sample - loss: 0.0507 - acc: 0.5925 - val_loss: 0.2057 - val_acc: 0.5219\nEpoch 96/300\n6621/6621 [==============================] - 2s 313us/sample - loss: 0.0507 - acc: 0.5850 - val_loss: 0.2056 - val_acc: 0.5225\nEpoch 97/300\n6621/6621 [==============================] - 2s 332us/sample - loss: 0.0506 - acc: 0.5925 - val_loss: 0.2056 - val_acc: 0.5231\nEpoch 98/300\n6621/6621 [==============================] - 2s 319us/sample - loss: 0.0505 - acc: 0.5975 - val_loss: 0.2055 - val_acc: 0.5281\nEpoch 99/300\n6621/6621 [==============================] - 2s 312us/sample - loss: 0.0506 - acc: 0.6050 - val_loss: 0.2055 - val_acc: 0.5306\nEpoch 100/300\n6621/6621 [==============================] - 2s 338us/sample - loss: 0.0505 - acc: 0.6150 - val_loss: 0.2054 - val_acc: 0.5294\nEpoch 101/300\n6621/6621 [==============================] - 2s 321us/sample - loss: 0.0504 - acc: 0.6025 - val_loss: 0.2054 - val_acc: 0.5306\nEpoch 102/300\n6621/6621 [==============================] - 2s 336us/sample - loss: 0.0504 - acc: 0.6225 - val_loss: 0.2053 - val_acc: 0.5288\nEpoch 103/300\n6621/6621 [==============================] - 2s 308us/sample - loss: 0.0504 - acc: 0.6100 - val_loss: 0.2053 - val_acc: 0.5312\nEpoch 104/300\n6621/6621 [==============================] - 2s 308us/sample - loss: 0.0503 - acc: 0.6150 - val_loss: 0.2052 - val_acc: 0.5331\nEpoch 105/300\n6621/6621 [==============================] - 2s 341us/sample - loss: 0.0503 - acc: 0.6100 - val_loss: 0.2052 - val_acc: 0.5325\nEpoch 106/300\n6621/6621 [==============================] - 2s 343us/sample - loss: 0.0503 - acc: 0.6200 - val_loss: 0.2051 - val_acc: 0.5306\nEpoch 107/300\n6621/6621 [==============================] - 2s 333us/sample - loss: 0.0502 - acc: 0.6200 - val_loss: 0.2051 - val_acc: 0.5319\nEpoch 108/300\n6621/6621 [==============================] - 2s 307us/sample - loss: 0.0502 - acc: 0.6325 - val_loss: 0.2051 - val_acc: 0.5312\nEpoch 109/300\n6621/6621 [==============================] - 2s 341us/sample - loss: 0.0502 - acc: 0.6300 - val_loss: 0.2050 - val_acc: 0.5325\nEpoch 110/300\n6621/6621 [==============================] - 2s 313us/sample - loss: 0.0501 - acc: 0.6225 - val_loss: 0.2050 - val_acc: 0.5350\nEpoch 111/300\n6621/6621 [==============================] - 2s 318us/sample - loss: 0.0500 - acc: 0.6425 - val_loss: 0.2049 - val_acc: 0.5337\nEpoch 112/300\n6621/6621 [==============================] - 2s 333us/sample - loss: 0.0500 - acc: 0.6575 - val_loss: 0.2049 - val_acc: 0.5350\nEpoch 113/300\n6621/6621 [==============================] - 2s 311us/sample - loss: 0.0500 - acc: 0.6525 - val_loss: 0.2049 - val_acc: 0.5344\nEpoch 114/300\n6621/6621 [==============================] - 2s 342us/sample - loss: 0.0499 - acc: 0.6550 - val_loss: 0.2048 - val_acc: 0.5356\nEpoch 115/300\n6621/6621 [==============================] - 2s 310us/sample - loss: 0.0499 - acc: 0.6650 - val_loss: 0.2048 - val_acc: 0.5344\nEpoch 116/300\n6621/6621 [==============================] - 2s 315us/sample - loss: 0.0498 - acc: 0.6600 - val_loss: 0.2048 - val_acc: 0.5369\nEpoch 117/300\n6621/6621 [==============================] - 2s 311us/sample - loss: 0.0497 - acc: 0.6775 - val_loss: 0.2047 - val_acc: 0.5375\nEpoch 118/300\n6621/6621 [==============================] - 2s 310us/sample - loss: 0.0497 - acc: 0.6625 - val_loss: 0.2047 - val_acc: 0.5362\nEpoch 119/300\n6621/6621 [==============================] - 2s 338us/sample - loss: 0.0497 - acc: 0.6550 - val_loss: 0.2047 - val_acc: 0.5387\nEpoch 120/300\n6621/6621 [==============================] - 2s 321us/sample - loss: 0.0496 - acc: 0.6725 - val_loss: 0.2046 - val_acc: 0.5412\nEpoch 121/300\n6621/6621 [==============================] - 2s 333us/sample - loss: 0.0496 - acc: 0.6650 - val_loss: 0.2046 - val_acc: 0.5412\nEpoch 122/300\n6621/6621 [==============================] - 2s 317us/sample - loss: 0.0496 - acc: 0.6850 - val_loss: 0.2046 - val_acc: 0.5400\nEpoch 123/300\n6621/6621 [==============================] - 2s 309us/sample - loss: 0.0495 - acc: 0.6800 - val_loss: 0.2045 - val_acc: 0.5406\nEpoch 124/300\n6621/6621 [==============================] - 2s 343us/sample - loss: 0.0494 - acc: 0.6850 - val_loss: 0.2045 - val_acc: 0.5400\nEpoch 125/300\n6621/6621 [==============================] - 2s 319us/sample - loss: 0.0494 - acc: 0.6800 - val_loss: 0.2045 - val_acc: 0.5375\nEpoch 126/300\n6621/6621 [==============================] - 2s 376us/sample - loss: 0.0493 - acc: 0.6800 - val_loss: 0.2044 - val_acc: 0.5375\nEpoch 127/300\n6621/6621 [==============================] - 3s 382us/sample - loss: 0.0493 - acc: 0.6750 - val_loss: 0.2044 - val_acc: 0.5369\nEpoch 128/300\n6621/6621 [==============================] - 2s 370us/sample - loss: 0.0492 - acc: 0.6800 - val_loss: 0.2044 - val_acc: 0.5344\nEpoch 129/300\n6621/6621 [==============================] - 3s 391us/sample - loss: 0.0491 - acc: 0.6925 - val_loss: 0.2043 - val_acc: 0.5331\nEpoch 130/300\n6621/6621 [==============================] - 2s 353us/sample - loss: 0.0491 - acc: 0.6850 - val_loss: 0.2043 - val_acc: 0.5325\nEpoch 131/300\n6621/6621 [==============================] - 2s 359us/sample - loss: 0.0490 - acc: 0.6775 - val_loss: 0.2043 - val_acc: 0.5356\nEpoch 132/300\n6621/6621 [==============================] - 2s 315us/sample - loss: 0.0489 - acc: 0.6975 - val_loss: 0.2042 - val_acc: 0.5381\nEpoch 133/300\n6621/6621 [==============================] - 2s 311us/sample - loss: 0.0489 - acc: 0.6900 - val_loss: 0.2042 - val_acc: 0.5394\nEpoch 134/300\n6621/6621 [==============================] - 2s 349us/sample - loss: 0.0489 - acc: 0.7075 - val_loss: 0.2042 - val_acc: 0.5419\nEpoch 135/300\n6621/6621 [==============================] - 2s 320us/sample - loss: 0.0488 - acc: 0.7050 - val_loss: 0.2042 - val_acc: 0.5431\nEpoch 136/300\n6621/6621 [==============================] - 2s 340us/sample - loss: 0.0488 - acc: 0.7100 - val_loss: 0.2041 - val_acc: 0.5431\nEpoch 137/300\n6621/6621 [==============================] - 2s 314us/sample - loss: 0.0487 - acc: 0.7125 - val_loss: 0.2041 - val_acc: 0.5412\nEpoch 138/300\n6621/6621 [==============================] - 2s 314us/sample - loss: 0.0486 - acc: 0.7175 - val_loss: 0.2041 - val_acc: 0.5425\nEpoch 139/300\n6621/6621 [==============================] - 2s 320us/sample - loss: 0.0485 - acc: 0.7200 - val_loss: 0.2040 - val_acc: 0.5431\nEpoch 140/300\n6621/6621 [==============================] - 2s 320us/sample - loss: 0.0484 - acc: 0.7150 - val_loss: 0.2040 - val_acc: 0.5437\nEpoch 141/300\n6621/6621 [==============================] - 2s 335us/sample - loss: 0.0484 - acc: 0.7200 - val_loss: 0.2040 - val_acc: 0.5437\nEpoch 142/300\n6621/6621 [==============================] - 2s 314us/sample - loss: 0.0483 - acc: 0.7250 - val_loss: 0.2039 - val_acc: 0.5406\nEpoch 143/300\n6621/6621 [==============================] - 2s 345us/sample - loss: 0.0482 - acc: 0.7300 - val_loss: 0.2039 - val_acc: 0.5419\nEpoch 144/300\n6621/6621 [==============================] - 2s 315us/sample - loss: 0.0482 - acc: 0.7250 - val_loss: 0.2039 - val_acc: 0.5412\nEpoch 145/300\n6621/6621 [==============================] - 2s 323us/sample - loss: 0.0481 - acc: 0.7350 - val_loss: 0.2039 - val_acc: 0.5419\nEpoch 146/300\n6621/6621 [==============================] - 2s 337us/sample - loss: 0.0480 - acc: 0.7375 - val_loss: 0.2038 - val_acc: 0.5400\nEpoch 147/300\n6621/6621 [==============================] - 2s 314us/sample - loss: 0.0480 - acc: 0.7450 - val_loss: 0.2038 - val_acc: 0.5412\nEpoch 148/300\n6621/6621 [==============================] - 2s 353us/sample - loss: 0.0480 - acc: 0.7375 - val_loss: 0.2038 - val_acc: 0.5406\nEpoch 149/300\n6621/6621 [==============================] - 3s 389us/sample - loss: 0.0479 - acc: 0.7400 - val_loss: 0.2037 - val_acc: 0.5406\nEpoch 150/300\n6621/6621 [==============================] - 2s 341us/sample - loss: 0.0478 - acc: 0.7375 - val_loss: 0.2037 - val_acc: 0.5400\nEpoch 151/300\n6621/6621 [==============================] - 2s 313us/sample - loss: 0.0476 - acc: 0.7400 - val_loss: 0.2037 - val_acc: 0.5381\nEpoch 152/300\n6621/6621 [==============================] - 2s 316us/sample - loss: 0.0476 - acc: 0.7475 - val_loss: 0.2037 - val_acc: 0.5375\nEpoch 153/300\n6621/6621 [==============================] - 2s 353us/sample - loss: 0.0475 - acc: 0.7375 - val_loss: 0.2036 - val_acc: 0.5375\nEpoch 154/300\n6621/6621 [==============================] - 2s 324us/sample - loss: 0.0474 - acc: 0.7425 - val_loss: 0.2036 - val_acc: 0.5394\nEpoch 155/300\n6621/6621 [==============================] - 2s 356us/sample - loss: 0.0473 - acc: 0.7525 - val_loss: 0.2036 - val_acc: 0.5394\nEpoch 156/300\n6621/6621 [==============================] - 2s 318us/sample - loss: 0.0473 - acc: 0.7425 - val_loss: 0.2035 - val_acc: 0.5406\nEpoch 157/300\n6621/6621 [==============================] - 2s 314us/sample - loss: 0.0472 - acc: 0.7450 - val_loss: 0.2035 - val_acc: 0.5412\nEpoch 158/300\n6621/6621 [==============================] - 2s 358us/sample - loss: 0.0471 - acc: 0.7475 - val_loss: 0.2035 - val_acc: 0.5412\nEpoch 159/300\n6621/6621 [==============================] - 2s 321us/sample - loss: 0.0470 - acc: 0.7425 - val_loss: 0.2034 - val_acc: 0.5431\nEpoch 160/300\n6621/6621 [==============================] - 2s 336us/sample - loss: 0.0470 - acc: 0.7475 - val_loss: 0.2034 - val_acc: 0.5406\nEpoch 161/300\n6621/6621 [==============================] - 2s 316us/sample - loss: 0.0469 - acc: 0.7425 - val_loss: 0.2034 - val_acc: 0.5425\nEpoch 162/300\n6621/6621 [==============================] - 2s 338us/sample - loss: 0.0469 - acc: 0.7475 - val_loss: 0.2034 - val_acc: 0.5412\nEpoch 163/300\n6621/6621 [==============================] - 2s 372us/sample - loss: 0.0467 - acc: 0.7475 - val_loss: 0.2033 - val_acc: 0.5431\nEpoch 164/300\n6621/6621 [==============================] - 2s 316us/sample - loss: 0.0467 - acc: 0.7500 - val_loss: 0.2033 - val_acc: 0.5425\nEpoch 165/300\n6621/6621 [==============================] - 2s 346us/sample - loss: 0.0465 - acc: 0.7525 - val_loss: 0.2033 - val_acc: 0.5412\nEpoch 166/300\n6621/6621 [==============================] - 2s 319us/sample - loss: 0.0466 - acc: 0.7575 - val_loss: 0.2033 - val_acc: 0.5431\nEpoch 167/300\n6621/6621 [==============================] - 2s 322us/sample - loss: 0.0464 - acc: 0.7475 - val_loss: 0.2032 - val_acc: 0.5431\nEpoch 168/300\n6621/6621 [==============================] - 2s 353us/sample - loss: 0.0463 - acc: 0.7450 - val_loss: 0.2032 - val_acc: 0.5450\nEpoch 169/300\n6621/6621 [==============================] - 2s 326us/sample - loss: 0.0463 - acc: 0.7425 - val_loss: 0.2032 - val_acc: 0.5475\nEpoch 170/300\n6621/6621 [==============================] - 2s 344us/sample - loss: 0.0462 - acc: 0.7550 - val_loss: 0.2032 - val_acc: 0.5481\nEpoch 171/300\n6621/6621 [==============================] - 2s 352us/sample - loss: 0.0460 - acc: 0.7475 - val_loss: 0.2031 - val_acc: 0.5481\nEpoch 172/300\n6621/6621 [==============================] - 2s 356us/sample - loss: 0.0461 - acc: 0.7475 - val_loss: 0.2031 - val_acc: 0.5481\nEpoch 173/300\n6621/6621 [==============================] - 2s 328us/sample - loss: 0.0459 - acc: 0.7625 - val_loss: 0.2031 - val_acc: 0.5481\nEpoch 174/300\n6621/6621 [==============================] - 2s 316us/sample - loss: 0.0458 - acc: 0.7500 - val_loss: 0.2031 - val_acc: 0.5469\nEpoch 175/300\n6621/6621 [==============================] - 2s 343us/sample - loss: 0.0458 - acc: 0.7550 - val_loss: 0.2030 - val_acc: 0.5469\nEpoch 176/300\n6621/6621 [==============================] - 2s 328us/sample - loss: 0.0457 - acc: 0.7525 - val_loss: 0.2030 - val_acc: 0.5481\nEpoch 177/300\n6621/6621 [==============================] - 2s 352us/sample - loss: 0.0455 - acc: 0.7500 - val_loss: 0.2030 - val_acc: 0.5494\nEpoch 178/300\n6621/6621 [==============================] - 2s 322us/sample - loss: 0.0454 - acc: 0.7625 - val_loss: 0.2030 - val_acc: 0.5462\nEpoch 179/300\n6621/6621 [==============================] - 2s 328us/sample - loss: 0.0453 - acc: 0.7575 - val_loss: 0.2030 - val_acc: 0.5475\nEpoch 180/300\n6621/6621 [==============================] - 2s 347us/sample - loss: 0.0452 - acc: 0.7525 - val_loss: 0.2029 - val_acc: 0.5469\nEpoch 181/300\n6621/6621 [==============================] - 2s 317us/sample - loss: 0.0451 - acc: 0.7600 - val_loss: 0.2029 - val_acc: 0.5456\nEpoch 182/300\n6621/6621 [==============================] - 2s 347us/sample - loss: 0.0451 - acc: 0.7575 - val_loss: 0.2029 - val_acc: 0.5444\nEpoch 183/300\n6621/6621 [==============================] - 2s 376us/sample - loss: 0.0450 - acc: 0.7600 - val_loss: 0.2029 - val_acc: 0.5444\nEpoch 184/300\n6621/6621 [==============================] - 3s 405us/sample - loss: 0.0449 - acc: 0.7575 - val_loss: 0.2029 - val_acc: 0.5444\nEpoch 185/300\n6621/6621 [==============================] - 2s 327us/sample - loss: 0.0447 - acc: 0.7650 - val_loss: 0.2029 - val_acc: 0.5444\nEpoch 186/300\n6621/6621 [==============================] - 3s 399us/sample - loss: 0.0447 - acc: 0.7575 - val_loss: 0.2028 - val_acc: 0.5437\nEpoch 187/300\n6621/6621 [==============================] - 3s 415us/sample - loss: 0.0447 - acc: 0.7600 - val_loss: 0.2028 - val_acc: 0.5437\nEpoch 188/300\n6621/6621 [==============================] - 3s 400us/sample - loss: 0.0444 - acc: 0.7650 - val_loss: 0.2028 - val_acc: 0.5444\nEpoch 189/300\n6621/6621 [==============================] - 3s 387us/sample - loss: 0.0443 - acc: 0.7725 - val_loss: 0.2028 - val_acc: 0.5444\nEpoch 190/300\n6621/6621 [==============================] - 2s 324us/sample - loss: 0.0442 - acc: 0.7675 - val_loss: 0.2028 - val_acc: 0.5450\nEpoch 191/300\n6621/6621 [==============================] - 2s 343us/sample - loss: 0.0442 - acc: 0.7725 - val_loss: 0.2028 - val_acc: 0.5437\nEpoch 192/300\n6621/6621 [==============================] - 2s 367us/sample - loss: 0.0441 - acc: 0.7575 - val_loss: 0.2028 - val_acc: 0.5431\nEpoch 193/300\n6621/6621 [==============================] - 2s 332us/sample - loss: 0.0439 - acc: 0.7700 - val_loss: 0.2028 - val_acc: 0.5444\nEpoch 194/300\n6621/6621 [==============================] - 2s 353us/sample - loss: 0.0439 - acc: 0.7725 - val_loss: 0.2027 - val_acc: 0.5444\nEpoch 195/300\n6621/6621 [==============================] - 2s 336us/sample - loss: 0.0438 - acc: 0.7600 - val_loss: 0.2027 - val_acc: 0.5437\nEpoch 196/300\n6621/6621 [==============================] - 2s 367us/sample - loss: 0.0438 - acc: 0.7775 - val_loss: 0.2027 - val_acc: 0.5444\nEpoch 197/300\n6621/6621 [==============================] - 4s 656us/sample - loss: 0.0436 - acc: 0.7625 - val_loss: 0.2027 - val_acc: 0.5444\nEpoch 198/300\n6621/6621 [==============================] - 4s 559us/sample - loss: 0.0435 - acc: 0.7625 - val_loss: 0.2027 - val_acc: 0.5450\nEpoch 199/300\n6621/6621 [==============================] - 5s 684us/sample - loss: 0.0433 - acc: 0.7700 - val_loss: 0.2027 - val_acc: 0.5456\nEpoch 200/300\n6621/6621 [==============================] - 3s 468us/sample - loss: 0.0433 - acc: 0.7675 - val_loss: 0.2027 - val_acc: 0.5456\nEpoch 201/300\n6621/6621 [==============================] - 2s 321us/sample - loss: 0.0433 - acc: 0.7700 - val_loss: 0.2027 - val_acc: 0.5456\nEpoch 202/300\n6621/6621 [==============================] - 3s 464us/sample - loss: 0.0431 - acc: 0.7700 - val_loss: 0.2027 - val_acc: 0.5456\nEpoch 203/300\n6621/6621 [==============================] - 4s 572us/sample - loss: 0.0431 - acc: 0.7625 - val_loss: 0.2027 - val_acc: 0.5450\nEpoch 204/300\n6621/6621 [==============================] - 6s 867us/sample - loss: 0.0429 - acc: 0.7625 - val_loss: 0.2027 - val_acc: 0.5462\nEpoch 205/300\n6621/6621 [==============================] - 5s 712us/sample - loss: 0.0428 - acc: 0.7700 - val_loss: 0.2027 - val_acc: 0.5450\nEpoch 206/300\n6621/6621 [==============================] - 3s 447us/sample - loss: 0.0427 - acc: 0.7775 - val_loss: 0.2027 - val_acc: 0.5444\nEpoch 207/300\n6621/6621 [==============================] - 2s 364us/sample - loss: 0.0427 - acc: 0.7725 - val_loss: 0.2027 - val_acc: 0.5444\nEpoch 208/300\n6621/6621 [==============================] - 3s 519us/sample - loss: 0.0425 - acc: 0.7675 - val_loss: 0.2027 - val_acc: 0.5444\nEpoch 209/300\n6621/6621 [==============================] - 5s 814us/sample - loss: 0.0425 - acc: 0.7650 - val_loss: 0.2027 - val_acc: 0.5444\nEpoch 210/300\n6621/6621 [==============================] - 4s 560us/sample - loss: 0.0422 - acc: 0.7775 - val_loss: 0.2027 - val_acc: 0.5456\nEpoch 211/300\n6621/6621 [==============================] - 2s 347us/sample - loss: 0.0422 - acc: 0.7725 - val_loss: 0.2027 - val_acc: 0.5469\nEpoch 212/300\n6621/6621 [==============================] - 2s 307us/sample - loss: 0.0421 - acc: 0.7775 - val_loss: 0.2028 - val_acc: 0.5469\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x229554022e8>"
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# Prepare data\n",
    "validation_data = ([X, A], y, val_mask)\n",
    "\n",
    "# Train model\n",
    "model.fit([X, A],\n",
    "          y,\n",
    "          sample_weight=train_mask,\n",
    "          epochs=300,\n",
    "          batch_size=N, #batch size = no of nodes. Put all nodes into neural network at once.\n",
    "          validation_data=validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[\n",
    "              EarlyStopping(patience=10,  restore_best_weights=True)\n",
    "          ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\n6621/6621 [==============================] - 1s 123us/sample - loss: 0.6190 - acc: 0.5700\nDone.\nTest loss: 0.6190159320831299\nTest accuracy: 0.5700064897537231\n"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "eval_results = model.evaluate([X, A],\n",
    "                              y,\n",
    "                              sample_weight=test_mask,\n",
    "                              batch_size=N)\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "      'Test accuracy: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_result = model.predict([X,A], batch_size=N)\n",
    "# y_group = []\n",
    "# for index, item in enumerate(y_result):\n",
    "#     y_group.append(np.argmax(y_result[index]))\n",
    "y_group = np.argmax(y_result, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.042012659002485554\n0.022234073748231345\n0.5760459145144238\n"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.adjusted_rand_score(ground_truth, y_group))\n",
    "print(metrics.adjusted_mutual_info_score(ground_truth, y_group))\n",
    "print(metrics.accuracy_score(ground_truth, y_group))\n",
    "#print(ground_truth)\n",
    "#print(y_group)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36664bitbaseconda80ab881b08fc4123973af10a07e0b9cc",
   "display_name": "Python 3.6.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}