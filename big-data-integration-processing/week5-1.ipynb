{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL exercises.\n",
    "### These exercises are modified from spark-sql/SparkSQL.ipynb to suit win10 environment without jdfc. All data will be loaded from csv directly.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "# Create a spark configuration with 20 threads.\n",
    "# This code will run locally on master\n",
    "conf = (SparkConf ()\n",
    "        . setMaster(\"local[20]\")\n",
    "        . setAppName(\"sample app for reading files\")\n",
    "        . set(\"spark.executor.memory\", \"2g\"))\n",
    "\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 methods which can load csv files into spark.\n",
    "- load csv file by `sc.textfile` and return a spark rdd object.\n",
    "- load csv file by `sqlContext.read.load` and return a spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1. Load the csv into RDD format.\n",
    "\n",
    "#csv_2_rdd = sc.textFile(\"big-data-3/gameclicks.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------------+-------+------+-------------+-----+------+---------+\n|          timestamp|clickId|userId|userSessionId|isHit|teamId|teamLevel|\n+-------------------+-------+------+-------------+-----+------+---------+\n|2016-05-26 15:06:55|    105|  1038|         5916|    0|    25|        1|\n|2016-05-26 15:07:09|    154|  1099|         5898|    0|    44|        1|\n|2016-05-26 15:07:14|    229|   899|         5757|    0|    71|        1|\n|2016-05-26 15:07:14|    322|  2197|         5854|    0|    99|        1|\n|2016-05-26 15:07:20|     22|  1362|         5739|    0|    13|        1|\n|2016-05-26 15:07:27|    107|  1071|         5939|    0|    27|        1|\n|2016-05-26 15:07:30|    289|  2359|         5764|    0|    85|        1|\n|2016-05-26 15:07:30|    301|  1243|         5900|    0|    86|        1|\n|2016-05-26 15:07:47|    274|  1628|         5896|    0|    82|        1|\n|2016-05-26 15:07:48|     66|   453|         5662|    0|    20|        1|\n|2016-05-26 15:07:49|    124|  2336|         5761|    0|    32|        1|\n|2016-05-26 15:07:53|    204|  1664|         5831|    0|    69|        1|\n|2016-05-26 15:07:58|    162|   243|         5943|    0|    54|        1|\n|2016-05-26 15:08:02|    308|  2333|         5701|    0|    90|        1|\n|2016-05-26 15:08:03|    160|   181|         5787|    0|    53|        1|\n|2016-05-26 15:08:04|    183|   212|         5756|    0|    63|        1|\n|2016-05-26 15:08:12|    314|  1142|         5871|    0|    93|        1|\n|2016-05-26 15:08:13|    171|  1085|         5882|    0|    61|        1|\n|2016-05-26 15:08:16|     67|  1215|         5905|    0|    20|        1|\n|2016-05-26 15:08:23|    332|  2306|         5783|    0|   100|        1|\n+-------------------+-------+------+-------------+-----+------+---------+\nonly showing top 20 rows\n\n"
    }
   ],
   "source": [
    "#Method 2. Load the csv into Spark dataframe.\n",
    "#print it\n",
    "sqlContext = SQLContext (sc)\n",
    "\n",
    "csv_2_df = sqlContext.read.load (\"big-data-3/game-clicks.csv\",\n",
    "                               format='com.databricks.spark.csv',\n",
    "                               header='true',\n",
    "                               inferSchema='true')\n",
    "\n",
    "csv_2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- timestamp: string (nullable = true)\n |-- clickId: integer (nullable = true)\n |-- userId: integer (nullable = true)\n |-- userSessionId: integer (nullable = true)\n |-- isHit: integer (nullable = true)\n |-- teamId: integer (nullable = true)\n |-- teamLevel: integer (nullable = true)\n\n"
    }
   ],
   "source": [
    "csv_2_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "755806"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "csv_2_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-------------------+-------+------+-------------+-----+------+---------+\n|          timestamp|clickId|userId|userSessionId|isHit|teamId|teamLevel|\n+-------------------+-------+------+-------------+-----+------+---------+\n|2016-05-26 15:06:55|    105|  1038|         5916|    0|    25|        1|\n|2016-05-26 15:07:09|    154|  1099|         5898|    0|    44|        1|\n|2016-05-26 15:07:14|    229|   899|         5757|    0|    71|        1|\n|2016-05-26 15:07:14|    322|  2197|         5854|    0|    99|        1|\n|2016-05-26 15:07:20|     22|  1362|         5739|    0|    13|        1|\n+-------------------+-------+------+-------------+-----+------+---------+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "csv_2_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+---------+\n|userid|teamlevel|\n+------+---------+\n|  1038|        1|\n|  1099|        1|\n|   899|        1|\n|  2197|        1|\n|  1362|        1|\n+------+---------+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "csv_2_df.select(\"userid\", \"teamlevel\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+---------+\n|userid|teamlevel|\n+------+---------+\n|  1513|        2|\n|   868|        2|\n|  1453|        2|\n|  1282|        2|\n|  1473|        2|\n+------+---------+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "csv_2_df.filter(csv_2_df[\"teamlevel\"] > 1).select(\"userid\", \"teamlevel\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+-----+------+\n|ishit| count|\n+-----+------+\n|    1| 83383|\n|    0|672423|\n+-----+------+\n\n"
    }
   ],
   "source": [
    "csv_2_df.groupBy(\"ishit\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------------+----------+\n|        avg(ishit)|sum(ishit)|\n+------------------+----------+\n|0.1103232840173272|     83383|\n+------------------+----------+\n\n"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "csv_2_df.select(mean('ishit'), sum(\"ishit\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load another table from another csv using the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_3_df = sqlContext.read.load (\"big-data-3/ad-clicks.csv\",\n",
    "                               format='com.databricks.spark.csv',\n",
    "                               header='true',\n",
    "                               inferSchema='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- timestamp: string (nullable = true)\n |-- txId: integer (nullable = true)\n |-- userSessionId: integer (nullable = true)\n |-- teamId: integer (nullable = true)\n |-- userId: integer (nullable = true)\n |-- adId: integer (nullable = true)\n |-- adCategory: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "csv_3_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge table by declaring join and common attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = csv_2_df.join(csv_3_df, 'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------+-------------------+-------+-------------+-----+------+---------+-------------------+-----+-------------+------+----+-----------+\n|userId|          timestamp|clickId|userSessionId|isHit|teamId|teamLevel|          timestamp| txId|userSessionId|teamId|adId| adCategory|\n+------+-------------------+-------+-------------+-----+------+---------+-------------------+-----+-------------+------+----+-----------+\n|  1362|2016-05-26 15:07:20|     22|         5739|    0|    13|        1|2016-06-16 10:21:01|39733|        34223|    13|   1|     sports|\n|  1362|2016-05-26 15:07:20|     22|         5739|    0|    13|        1|2016-06-15 23:52:15|38854|        34223|    13|   3|electronics|\n|  1362|2016-05-26 15:07:20|     22|         5739|    0|    13|        1|2016-06-15 12:23:31|37940|        34223|    13|  15|     sports|\n|  1362|2016-05-26 15:07:20|     22|         5739|    0|    13|        1|2016-06-13 00:12:01|32627|        26427|    13|  14|    fashion|\n|  1362|2016-05-26 15:07:20|     22|         5739|    0|    13|        1|2016-06-12 13:02:36|31729|        26427|    13|   4|      games|\n+------+-------------------+-------+-------------+-----+------+---------+-------------------+-----+-------------+------+----+-----------+\nonly showing top 5 rows\n\n"
    }
   ],
   "source": [
    "merge.show(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36664bitalexlvirtualenvb9f0b0a3af2a4e06a89ee778b9503914",
   "display_name": "Python 3.6.6 64-bit ('alexl': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}